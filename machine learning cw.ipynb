{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ecf523ca8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m \u001b[0;31m#Ensure seaborn is installed for use on dcs machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL \n",
    "import pandas as pd\n",
    "import seaborn as sns #Ensure seaborn is installed for use on dcs machine\n",
    "import random\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Load all the images\n",
    "dat = pd.read_csv('accent-mfcc-data-1.data', header=None)\n",
    "\n",
    "dat_str = dat[0] #Get y column\n",
    "datY = np.zeros(dat_str.shape[0])\n",
    "for i in range(0,datY.shape[0]): #Converts symbols to numbers\n",
    "    if dat_str[i] == 'FR':\n",
    "        datY[i] = 1\n",
    "    elif dat_str[i] == 'GE':\n",
    "        datY[i]=2\n",
    "    elif dat_str[i] == 'IT':\n",
    "        datY[i]=3\n",
    "    elif dat_str[i] == 'UK':\n",
    "        datY[i]=4\n",
    "    elif dat_str[i] == 'US':\n",
    "        datY[i] = 5\n",
    "    else:\n",
    "        datY[i] = 0\n",
    "\n",
    "#Data is not linearly seperable in Base data set\n",
    "        \n",
    "datX = dat.loc[:,1:]#Get rest of data\n",
    "\n",
    "sizeX = datX.shape[0]\n",
    "\n",
    "print (\"Total num: \",sizeX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPca(X, num): #Function to obtain PCA\n",
    "    mean = np.mean(X, axis=0)\n",
    "    dat_centred = X - mean\n",
    "    U, S, V = np.linalg.svd(dat_centred, full_matrices=True)\n",
    "    comps = V[:num]\n",
    "    return  comps, mean, dat_centred\n",
    "\n",
    "top_2, _, _ = getPca(datX, 2) #Get PCA for 2 dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean = np.mean(datX, axis=0)\n",
    "dat_centred = datX-mean\n",
    "\n",
    "PcaTransformed2 = np.dot(dat_centred, top_2.T)#Transforms the Pca\n",
    "\n",
    "\n",
    "labels=datY\n",
    "colours={0:'red',1:'green',2:'blue',3:'yellow',4:'black',5:'purple'}\n",
    "labl={0:'ES', 1:'FR', 2:'GE', 3:'IT', 4:'UK', 5:'US'}\n",
    "fig,ax=plt.subplots(figsize=(8,8))\n",
    "for i in np.unique(labels): #Plot every point\n",
    "    ix=np.where(labels==i)\n",
    "    ax.scatter(PcaTransformed2[:, 0][ix],PcaTransformed2[:, 1][ix],s=20 ,c=colours[i],label=labl[i])\n",
    "    \n",
    "plt.xlabel(\"First PC\",fontsize=15)#Label axes\n",
    "plt.ylabel(\"Second PC\",fontsize=15)\n",
    "\n",
    "plt.legend() \n",
    "plt.show()#Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evidently, the data is no linearly seperable in 2 dimenions given how much the clusters in the left overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Old function before discovering euclidean_distances\n",
    "#def getKernel(x,gamma):\n",
    "#    X = x.to_numpy()\n",
    "#    K = np.zeros((x.shape[0],x.shape[0]))\n",
    "#    for i in range(0,X.shape[0]):\n",
    "#        for j in range(0,X.shape[0]):\n",
    "#            n = (np.linalg.norm(X[i,:]-X[j,:]))\n",
    "#            K[i][j] = np.exp(-gamma * (n * n))\n",
    "#    A = np.full((sizeX,sizeX),1/sizeX)\n",
    "#    AK = np.matmul(A,K) #Saves having to compute AK twice\n",
    "#    Knorm = np.add(np.subtract(np.subtract(K,AK),np.matmul(K,A)),np.matmul(AK,A))\n",
    "#    return Knorm\n",
    "\n",
    "\n",
    "def getKernel(x,gamma): #Function to find kernel\n",
    "    X = x.to_numpy()\n",
    "    dist = euclidean_distances(X, X, squared=True) #Gets euclidean distance for kernels\n",
    "    K = np.exp(-gamma * dist) #Sets kernel value\n",
    "    A = np.full((sizeX,sizeX),1/sizeX)\n",
    "    AK = np.matmul(A,K) #Saves having to compute AK twice\n",
    "    Knorm = np.add(np.subtract(np.subtract(K,AK),np.matmul(K,A)),np.matmul(AK,A)) #Get normalised K matrix\n",
    "    return Knorm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Perceptron(x,y,max_steps):\n",
    "    step = 0\n",
    "    tot_steps = 0\n",
    "    tot_errors = 0\n",
    "    errors = 1\n",
    "    w = np.ones((len(x[1]),np.argmax(y) + 1)) #Initialise weights to 1\n",
    "    x_arr = np.array(x)\n",
    "    while(step < max_steps):     #Run for each step \n",
    "        errors = 0\n",
    "        \n",
    "        shuffler = np.random.permutation(len(x))#Randomize to reduce err\n",
    "        x_arr = x_arr[shuffler]\n",
    "        y = y[shuffler]\n",
    "        for i in range (len(x)): #Calculates prediction for each x row\n",
    "            xi = x_arr[i][0:len(x[1])]     \n",
    "            yi = y[i]\n",
    "\n",
    "            y_pred = np.argmax(np.matmul(xi,w)) \n",
    "\n",
    "            if (y_pred != yi):     \n",
    "                for j in range (len(w)):#Updates w and increments errors if prediction is incorrect\n",
    "\n",
    "                    w[j][int(yi)] += xi[j]          \n",
    "                    w[j][int(y_pred)] -= xi[j]     \n",
    "                errors += 1\n",
    "                tot_errors += 1\n",
    "            tot_steps += 1\n",
    "\n",
    "        step = step + 1 \n",
    "    #check errors after all steps\n",
    "    for i in range (len(x)):   \n",
    "        xi = x_arr[i][0:len(x[1])]     \n",
    "        yi = y[i]\n",
    "        y_pred = np.argmax(np.matmul(xi,w)) \n",
    "        if (y_pred != yi):         \n",
    "            errors += 1\n",
    "            tot_errors += 1\n",
    "    return w,1-(tot_errors/tot_steps),errors\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variance = np.var(datX, axis=0).to_numpy()\n",
    "\n",
    "max_var = np.max(variance)\n",
    "min_var = np.min(variance)\n",
    "gammax = 1 / (2 * ((min_var) - 0.05))\n",
    "gammin = 1 / (2 * ((max_var) + 0.05)) #Calculate min and max gamma\n",
    "\n",
    "mean = np.mean(datX, axis=0)\n",
    "dat_centred = datX - mean\n",
    "gammas = []\n",
    "nums = []\n",
    "\n",
    "g = gammin\n",
    "\n",
    "while g < gammax:\n",
    "    gammas.append(g)\n",
    "    g+=0.01\n",
    "    \n",
    "increment = 10 #Increases principle components by 10 each time. Can be lowered.\n",
    "\n",
    "for i in range(0,329,increment):\n",
    "    nums.append(i)\n",
    "\n",
    "    \n",
    "errors = np.zeros((len(nums),len(gammas))) #Store for errors for each hyperparameter combo\n",
    "g = 0\n",
    "\n",
    "\n",
    "while gammin < gammax: #Grid search\n",
    "    Ker = getKernel(dat_centred,gammin) #Get kernel for current gamma\n",
    "    n=0\n",
    "    for i in range(2,sizeX,increment):\n",
    "        print(i)\n",
    "        PCA, _, _ = getPca(Ker,i)\n",
    "        transformed = np.dot(Ker, PCA.T)#Transform pca for use in perceptron\n",
    "        transformed = np.append(np.ones((len(transformed),1)),transformed,axis=1) #Add bias column\n",
    "        w, acc, errs = Perceptron(transformed,datY,10)#Run perceptron to get errors\n",
    "        errors[n][g] = errs\n",
    "\n",
    "        n+=1#Next PC number\n",
    "    gammin += 0.01\n",
    "    g+=1\n",
    "    print(gammin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sns.set()\n",
    "    \n",
    "ax1 = hm = sns.heatmap(errors, xticklabels=gammas, yticklabels=nums) #Display heatmap with errors for each gamma and PC\n",
    "ax1.set(xlabel='Gamma', ylabel='Principle Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Data seems to be lineraly seperable for 20 + PCs, generating high number of errors for less."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
